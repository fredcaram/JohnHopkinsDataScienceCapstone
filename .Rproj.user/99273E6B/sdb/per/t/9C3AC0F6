{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Data science Capstone Project - Data Exploring\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n\n## Summary\n\nThis reports is aimed at the exploration of contents of 3 large data files, and their cleaning and preparation for a natural language algorithm to predict next-words. The ‘quanteda’ package is employed, because of the ease of use, what can compromise performance in a real situation. \n\nUntil the date of this report, the following steps were accomplished:\n\n* Understanding the size of the data, the number of lines, and characters per line, using quanteda.\n* Extraction of a sample for each document, to better understand the data and the methods used on them, without harm to the performance.\n* Striping excess whitespace and punctuation, converting words to lower case and filtering for profanity.\n* The documents were not stemmed in order to keep the context for the ngrams\n* Stop words were retained for 2 and 3 grams since they are needed to make sense of the data\n* Words with sparsity bigger than 75% sparsity in the documents are removed\n\n\n##Obtaining and undestanding the files\nThe files were obtaining throught this [link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)\n\n'LaF' library was used to determine the number of lines and to generate file samples of 50,000 lines chunk, and 'pander' for prettier output, the librarys loading and some variable declarations were left out to make the code more readable.\n```{r, echo=FALSE, include=FALSE}\nlibrary(LaF)\nlibrary(pander)\nlibrary(tools)\nlibrary(quanteda)\nlibrary(RColorBrewer)\nlibrary(ggplot2)\n\nnlines <- c()\nfiles_size <- c()\npath <- \"./data/final/en_US/\"\nsample_path <- \"./data/final/en_US/samples/\"\n```\n```{r, cache=TRUE}\n#Get only txt files\nfile_list <- list.files(path, pattern = \"^.*[.]txt$\")\nfor(filename in file_list){\n  filepath <- paste0(path, filename)\n  files_size <- c(files_size, file.size(filepath))\n  line_number <- determine_nlines(filepath)\n  nlines <- c(nlines, line_number)\n  #Generate the sample\n  sample_lines(paste0(sample_path,filename),  as.integer(line_number * 0.1), nlines = line_number)\n}\nfiles_info <- data.frame(file_list, nlines, files_size)\nnames(files_info) <- c(\"File\", \"Number of Lines\", \"File Size\")\npander(files_info)\n```\n\n\n## Words exploration\n\nSince the full files are a little bit to heavy for further analysis we are going to use the sample files\n```{r, warning=FALSE, message=FALSE, cache=TRUE}\n\nsample_corp <- corpus(textfile(file=paste0(sample_path, \"*\")))\nsummary(sample_corp)\n```\n\nGet the bad words so we can take them out\n\nFor the words analysis, the stop words and the bad words were taken away, as well as the punctuation and the numbers.\n```{r, warning=FALSE, message=FALSE, cache=TRUE}\n#taken from https://github.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\nbadWords <- VectorSource(readLines(\"./dirty/english.txt\"))\nignoredFeatures <- c(stopwords(\"english\"), badWords$content)\n```\n\nAs seen in the word cloud and in the table below, the most used words in the twitter sample were: just, like and get\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE, error=FALSE}\ntwitter_corp <- corpus(textfile(file=paste0(sample_path, \"en_US.twitter.txt\")))\ntwitter_dfm <- dfm(twitter_corp, ignoredFeatures=ignoredFeatures, verbose=FALSE)\npander(topfeatures(twitter_dfm, 10))\n```\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE, error=FALSE}\nplot(twitter_dfm, max.words=100, colors = brewer.pal(6, \"Dark2\"), scale=c(4, .2))\n```\n\nFor the blogs, the most common words were: one, will and just\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE, error=FALSE}\nblogs_corp <- corpus(textfile(file=paste0(sample_path, \"en_US.blogs.txt\")))\nblogs_dfm <- dfm(blogs_corp, ignoredFeatures=ignoredFeatures, verbose=FALSE)\npander(topfeatures(blogs_dfm, 10))\n```\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE}\nplot(blogs_dfm, max.words=100, colors = brewer.pal(6, \"Dark2\"), scale=c(4, .2))\n```\n\nAnd finally for the news, the most common words were said, will and one\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE}\nnews_corp <- corpus(textfile(file=paste0(sample_path, \"en_US.news.txt\")))\nnews_dfm <- dfm(news_corp, ignoredFeatures=ignoredFeatures, verbose=FALSE)\npander(topfeatures(news_dfm, 10))\n```\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE}\nplot(news_dfm, max.words=100, colors = brewer.pal(6, \"Dark2\"), scale=c(4, .2))\n```\n\nSo, even thought there were some differences between the message channel, there were also many similarities, it is possible to notice that the words \"one, will and just\"\" repeated on the top 3 words for at least two of the mediums and also happens when we look at the consolidated features.\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE, error=FALSE}\nsample_dfm <- dfm(sample_corp, ignoredFeatures=ignoredFeatures, verbose=FALSE)\npander(topfeatures(sample_dfm, 10))\n```\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE}\nplot(sample_dfm, max.words=100, colors = brewer.pal(6, \"Dark2\"), scale=c(4, .2))\n```\n\nIt is also possible to notice that a few word respond for most of the terms\n```{r}\ntermfreq <- sort(colSums(sample_dfm), decreasing = TRUE)\ntotalfreq <- sum(termfreq)\ntotalcount <- length(termfreq)\ntermfreq_df <- data.frame(word=names(termfreq),freq=termfreq,cumfreq=cumsum(termfreq), perccumfreq=cumsum(termfreq)/totalfreq, rank=seq_along(termfreq), perccont=seq_along(termfreq)/totalcount)\nggplot(termfreq_df, aes(perccont, perccumfreq)) + geom_point() + xlab(\"Cumulative word count\") + ylab(\"Cumulative word frequency\")\n```\n\n\n## Ngrams exploration\n\nFor the ngrams i've kepts the stopwords, since they are important for the context and as expected its is possible to see that most of the top bigrams and trigrams contains them.\n\n**Bigram**\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE, error=FALSE}\nbigram_dfm <- dfm(sample_corp, ignoredFeatures=badWords$content, ngrams=2, verbose=FALSE)\npander(topfeatures(bigram_dfm, 10))\n```\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE}\nplot(bigram_dfm, max.words=100, colors = brewer.pal(6, \"Dark2\"), scale=c(4, .2))\n```\n\n**Trigram**\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE, error=FALSE}\ntrigram_dfm <- dfm(sample_corp, ignoredFeatures=badWords$content, ngrams=3, verbose=FALSE)\npander(topfeatures(trigram_dfm, 10))\n```\n```{r, warning=FALSE, message=FALSE, cache=TRUE, comment=FALSE}\nplot(trigram_dfm, max.words=100, colors = brewer.pal(6, \"Dark2\"), scale=c(4, .2))\n```\n\nIt is also possible to notice that as the \"n\" increases, the frequency of terms reduces, but there is still a big concentration of most used terms as shown in the graphs below:\n\n**Bigram**\n```{r, cache=TRUE,}\nbigramfreq <- sort(colSums(bigram_dfm), decreasing = TRUE)\ntotalbigramfreq <- sum(bigramfreq)\ntotalbigramcount <- length(bigramfreq)\nbigramfreq_df <- data.frame(word=names(bigramfreq),freq=bigramfreq, perccumfreq=cumsum(bigramfreq)/totalbigramfreq, perccont=seq_along(bigramfreq)/totalbigramcount)\nggplot(bigramfreq_df, aes(perccont, perccumfreq)) + geom_point() + xlab(\"Cumulative terms count\") + ylab(\"Cumulative terms frequency\")\n```\n\n**Trigram**\n```{r, cache=TRUE,}\ntrigramfreq <- sort(colSums(trigram_dfm), decreasing = TRUE)\ntotaltrigramfreq <- sum(trigramfreq)\ntotaltrugramcount <- length(trigramfreq)\ntrigramfreq_df <- data.frame(word=names(trigramfreq),freq=trigramfreq, perccumfreq=cumsum(trigramfreq)/totaltrigramfreq, perccont=seq_along(trigramfreq)/totaltrugramcount)\nggplot(trigramfreq_df, aes(perccont, perccumfreq)) + geom_point() + xlab(\"Cumulative terms count\") + ylab(\"Cumulative terms frequency\")\n```\n\n\n## Next steps and model consideration\n\nAs noticed during the exploration, there is a great concentration of frequency in \"few\" terms, so we can reduce the features and complexity of the model removing terms with only 1 ocurrence.\n\nI stil have to find a way to improve performance, since processing the whole files in 'quanteda' package stressed a lot my computer, 'tm' did a little better and may be my way to go.\n\nFor the model I will select the words based on the probability of ocurring in the unigrams, bigrams and trigrams but I still have to figure out the best way of weighting them. ",
    "created" : 1468770873181.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2653477150",
    "id" : "9C3AC0F6",
    "lastKnownWriteTime" : 1468772895,
    "last_content_update" : 1468772895493,
    "path" : "~/Documents/R/Projects/Data Science Capstone Project/Markdown_Files/Project_Markdown.Rmd",
    "project_path" : "Markdown_Files/Project_Markdown.Rmd",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}